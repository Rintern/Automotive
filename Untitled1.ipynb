{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a81bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from humanfriendly import format_timespan\n",
    "import json\n",
    "import requests\n",
    "import shutil \n",
    "import zipfile \n",
    "\n",
    "#image operation libraries\n",
    "import cv2\n",
    "from skimage import color,exposure,transform\n",
    "\n",
    "#data split and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#NN libraries \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2e1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(directory):\n",
    "    \"\"\"\n",
    "    create directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directorys path to create \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    for d in directory:\n",
    "        if os.path.isdir(d):\n",
    "            print(\"directory {} already exist\".format(d))\n",
    "        if os.path.isdir(d)==False:\n",
    "            os.mkdir(path=d)\n",
    "            print(\"directory {} created successfully\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192bac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_directory(directorys):\n",
    "    \"\"\"\n",
    "    delete directory \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directorys to deleate along with all the files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(directorys)>=1:\n",
    "        for d in directorys:\n",
    "            if os.path.isdir(d):\n",
    "                try:\n",
    "                    if os.path.isfile(d):\n",
    "                        os.remove(path=d)\n",
    "                    else:\n",
    "                        shutil.rmtree(path=d)\n",
    "                        print(\"Removed: {}\".format(d))\n",
    "                except:\n",
    "                    print(\"Failed to removed: {}\".format(d))\n",
    "            else:\n",
    "                print(\"Failed to removed: {}\".format(d))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d04adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_archive(base_name,root_dir,zip_format='zip'):\n",
    "    \"\"\"\n",
    "    created zip for given folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_name : name of zip file\n",
    "    root_dir : directory to archive/zip\n",
    "    zip_format : zip or tar \n",
    "        DESCRIPTION. The default is 'zip'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    shutil.make_archive(base_name=base_name, format=zip_format, root_dir=root_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0094c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: C:\\Users\\Hp\\BFSI\\data\n",
      "Failed to removed: C:\\Users\\Hp\\BFSI\\model\n",
      "Failed to removed: C:\\Users\\Hp\\BFSI\\label\n",
      "Failed to removed: C:\\Users\\Hp\\BFSI\\zip\n",
      "Failed to removed: C:\\Users\\Hp\\BFSI\\pyc_model\n",
      "directory C:\\Users\\Hp\\BFSI\\data created successfully\n",
      "directory C:\\Users\\Hp\\BFSI\\model created successfully\n",
      "directory C:\\Users\\Hp\\BFSI\\label created successfully\n",
      "directory C:\\Users\\Hp\\BFSI\\zip created successfully\n",
      "directory C:\\Users\\Hp\\BFSI\\pyc_model created successfully\n"
     ]
    }
   ],
   "source": [
    "\"Description : Create data, model and label folder\"\n",
    "data_path=os.path.join(os.getcwd(),\"data\")\n",
    "model_path=os.path.join(os.getcwd(),\"model\")\n",
    "label_path=os.path.join(os.getcwd(),\"label\")\n",
    "zip_path=os.path.join(os.getcwd(),\"zip\")\n",
    "pyc_model_path=os.path.join(os.getcwd(),\"pyc_model\")\n",
    "#deleting folder\n",
    "delete_directory(directorys=[data_path,model_path,label_path,zip_path,pyc_model_path])\n",
    "\n",
    "#creating folder\n",
    "make_directory([data_path,model_path,label_path,zip_path,pyc_model_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07b27e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img,input_shape=(48,48,3)):\n",
    "    \"\"\"\n",
    "    Description: perform preprocessing on given image\n",
    "    Args:\n",
    "        img: image\n",
    "        IMG_SIZE: shape of image you want after preprocessing \n",
    "        \n",
    "    Returns: image\n",
    "        \n",
    "    \"\"\"\n",
    "    # Histogram normalization in y\n",
    "    hsv = color.rgb2hsv(img)\n",
    "    hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "    img = color.hsv2rgb(hsv)\n",
    "\n",
    "    # central crop\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0]//2, img.shape[1]//2\n",
    "    img = img[centre[0]-min_side//2:centre[0]+min_side//2,\n",
    "              centre[1]-min_side//2:centre[1]+min_side//2,\n",
    "              :]\n",
    "\n",
    "    # rescale to standard size\n",
    "    img = transform.resize(img, input_shape)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2097ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path,classes=43):\n",
    "    '''\n",
    "    Description: load data and perform preprocessing  \n",
    "    Args:\n",
    "        file_path:path where data is present\n",
    "        classes: number of class where to load data\n",
    "    Returns: numpy format data and its label\n",
    "    '''\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in tqdm(range(classes)):\n",
    "        path = os.path.join(file_path,'Train',str(i))\n",
    "        images = os.listdir(path)\n",
    "        for a in images:\n",
    "            try:\n",
    "                img = cv2.imread(path + '\\\\'+ a)\n",
    "                img=preprocess_img(img)\n",
    "\n",
    "                data.append(img)\n",
    "                labels.append(i)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data),np.array(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753de5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_val_data(file_path):\n",
    "    '''\n",
    "    Description: load data and perform preprocessing  \n",
    "    Args:\n",
    "        file_path: path where to read validation data\n",
    "    Returns: numpy formata data and its label\n",
    "    '''\n",
    "    test = pd.read_csv(os.path.join(file_path,'Test.csv'))\n",
    "    test.head()\n",
    "    val_path=os.path.join(file_path,\"Test\")\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    for file_name, class_id  in tqdm(zip(list(test['Path']), list(test['ClassId']))):\n",
    "        try:\n",
    "            img_path = os.path.join(val_path,file_name[5:])\n",
    "            img=cv2.imread(img_path)\n",
    "            X_val.append(preprocess_img(img))\n",
    "            y_val.append(class_id)\n",
    "        except Exception as e:\n",
    "                    print(e)\n",
    "        \n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    return X_val,y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec17b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r'C:\\Users\\Hp\\OneDrive\\Desktop\\Demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5be9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(48,48,3)\n",
    "num_classes=43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b9ede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [17:59<00:00, 25.10s/it]\n"
     ]
    }
   ],
   "source": [
    "X,y=prepare_data(file_path,classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ce4678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f54ea75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=tf.keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test=tf.keras.utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5682de11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12630it [07:13, 29.16it/s]\n"
     ]
    }
   ],
   "source": [
    "X_val,y_val= Prepare_val_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf4c7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(input_shape=input_shape,num_classes=num_classes):\n",
    "    '''\n",
    "    Description:Create architecture of teal model\n",
    "    Args:\n",
    "        input_shape:input shape for model\n",
    "        num_classes:number of class for to have final layer\n",
    "    '''\n",
    "    model=tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu',input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=512,activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "    model.add(tf.keras.layers.Dense(num_classes,activation='softmax'))\n",
    "    \n",
    "    #compile the model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bffcc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,X_train, y_train, X_test, y_test,batch_size=16,epochs=5,filename='tsr_model'):\n",
    "    '''\n",
    "    Description:Training  model\n",
    "    Args:\n",
    "        model:model to train \n",
    "        X_train: X_train for training \n",
    "        X_test: for validation\n",
    "        y_train: label for X_train\n",
    "        y_test: label for X_test\n",
    "        batch_size: batch size for training model\n",
    "        epochs: number of epochs to train model \n",
    "        filename : name to save extracted model\n",
    "    '''    \n",
    "    # Callbacks\n",
    "    checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=filename+'.h5',monitor='val_loss',verbose=1,save_best_only=True,mode='auto')\n",
    "    ES=tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=5,mode='min',restore_best_weights=True)\n",
    "    \n",
    "    #train and record time for training\n",
    "    start_time=time.time()\n",
    "    history=model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,verbose=1, callbacks=[ES,checkpoint],validation_data=(X_test,y_test))#tensorboard_callback,,validation_split=0.05\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"Elapsed time: {}\".format(format_timespan(elapsed_time)))\n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9d39803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 23, 23, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                22059     \n",
      "=================================================================\n",
      "Total params: 1,358,155\n",
      "Trainable params: 1,358,155\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CNN_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9b569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31367 samples, validate on 7842 samples\n",
      "Epoch 1/5\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 1.2618 - accuracy: 0.6280\n",
      "Epoch 00001: val_loss improved from inf to 0.16555, saving model to tsr_model.h5\n",
      "31367/31367 [==============================] - 422s 13ms/sample - loss: 1.2616 - accuracy: 0.6280 - val_loss: 0.1656 - val_accuracy: 0.9492\n",
      "Epoch 2/5\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.9243\n",
      "Epoch 00002: val_loss improved from 0.16555 to 0.09788, saving model to tsr_model.h5\n",
      "31367/31367 [==============================] - 414s 13ms/sample - loss: 0.2427 - accuracy: 0.9243 - val_loss: 0.0979 - val_accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "31360/31367 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9525\n",
      "Epoch 00003: val_loss improved from 0.09788 to 0.09576, saving model to tsr_model.h5\n",
      "31367/31367 [==============================] - 504s 16ms/sample - loss: 0.1518 - accuracy: 0.9525 - val_loss: 0.0958 - val_accuracy: 0.9677\n",
      "Epoch 4/5\n",
      " 7072/31367 [=====>........................] - ETA: 6:59 - loss: 0.1304 - accuracy: 0.9599WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4d1a41e33f8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#,batch_size=32,epochs=20,filename='tsr_model')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-108e6f49e3f1>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, y_train, X_test, y_test, batch_size, epochs, filename)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#train and record time for training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mstart_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#tensorboard_callback,,validation_split=0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RahulB\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model,history=train_model(model,X_train, y_train, X_test, y_test)#,batch_size=32,epochs=20,filename='tsr_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b96da12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.30971797e-09, 1.99216331e-07, 1.41695065e-08, 1.42091230e-05,\n",
       "        2.93287280e-06, 5.76407074e-05, 4.29370983e-09, 6.92759932e-05,\n",
       "        7.05560865e-07, 2.54929546e-06, 2.96245339e-08, 2.27955335e-10,\n",
       "        3.42553458e-10, 5.39991607e-10, 3.67238573e-10, 4.63471528e-09,\n",
       "        9.99851704e-01, 2.84048804e-12, 8.04404720e-12, 2.50720495e-12,\n",
       "        2.05414130e-09, 3.37085416e-12, 1.65170588e-15, 1.25050209e-12,\n",
       "        4.24840978e-12, 4.03453243e-11, 3.02054978e-11, 2.97496999e-10,\n",
       "        1.20198046e-10, 6.08331948e-14, 9.93896155e-13, 1.87502439e-13,\n",
       "        5.79450210e-10, 1.53578039e-09, 2.19943595e-11, 8.25414581e-09,\n",
       "        2.09397430e-11, 4.17343352e-13, 4.80068025e-13, 4.17176450e-12,\n",
       "        5.20678270e-07, 2.18419172e-09, 1.85776472e-07],\n",
       "       [1.52889895e-10, 1.00000000e+00, 3.37393058e-10, 4.92586180e-12,\n",
       "        4.62631156e-10, 4.49560655e-09, 5.68357481e-13, 9.88309694e-13,\n",
       "        2.33964568e-13, 2.48140807e-20, 1.33875040e-15, 6.89989068e-18,\n",
       "        4.59078570e-16, 2.92591550e-17, 1.84614353e-21, 2.49627280e-15,\n",
       "        1.88003122e-19, 1.38667653e-19, 6.68122182e-20, 1.92145219e-20,\n",
       "        2.72243007e-17, 3.87382009e-21, 6.49137430e-18, 1.73427237e-20,\n",
       "        1.73365069e-20, 2.14093229e-16, 2.06638733e-18, 8.89642146e-24,\n",
       "        7.00612291e-16, 7.46603096e-17, 1.24262359e-21, 8.67385985e-20,\n",
       "        4.87899766e-19, 1.61842007e-20, 1.04356867e-19, 1.17441682e-20,\n",
       "        1.29616787e-20, 3.30277810e-21, 6.32500884e-18, 2.03520384e-19,\n",
       "        2.30634349e-14, 9.64748907e-21, 7.61431903e-20]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get prediction of model \n",
    "pred=model.predict(X_val)\n",
    "pred[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a3b3949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  1, 38, 33, 11, 38, 18, 12, 25, 35], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(pred,axis=-1)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08f5c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.946714172604909\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "acc=metrics.accuracy_score(y_true=y_val,y_pred=y_pred)\n",
    "print(\"Accuracy is : {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff3bba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Save data and label\n",
    "\"\"\"\n",
    "label=pd.DataFrame()\n",
    "img_name = []\n",
    "img_label = [] \n",
    "  \n",
    "for i in range(X_val.shape[0]):\n",
    "    cv2.imwrite(os.path.join(data_path,str(i)+\".jpg\") ,X_val[i]*255.0) # don't use plt.imread otheriwse while loading the saved images , and passing to model there is accuarcy drop \n",
    "    img_name.append(str(i)+\".jpg\")\n",
    "    img_label.append(y_val[i])\n",
    "label['image'] = img_name\n",
    "label[\"label\"] = np.array(img_label)\n",
    "\n",
    "#write orig_label dataframe\n",
    "label.to_csv(os.path.join(label_path,\"label.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad3f3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Zip data\n",
    "\"\"\"\n",
    "make_archive(base_name=os.path.join(zip_path,\"data\"),root_dir=data_path,zip_format='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e81e3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Zip label\n",
    "\"\"\"\n",
    "make_archive(base_name=os.path.join(zip_path,\"label\"),root_dir=label_path,zip_format='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf9710e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encryption=0 #0 if model is uploaded directly as a zip, 1 if model is encryted as .pyc and uploaded as a zip\n",
    "if os.path.isfile(os.path.join(zip_path,\"model.zip\")):\n",
    "    delete_directory(directorys=[os.path.join(zip_path,\"model.zip\")])\n",
    "if model_encryption:\n",
    "    make_archive(base_name=os.path.join(zip_path,\"model\"),root_dir=pyc_model_path,zip_format='zip')\n",
    "else:\n",
    "    make_archive(base_name=os.path.join(zip_path,\"model\"),root_dir=model_path,zip_format='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: AIShield API URL and subscription key\n",
    "\"\"\" \n",
    "# url=\"https://apim-prod-ais-01.azure-api.net/trial/ic/ais/ImageClassification/VulnerabiltyReport\"\n",
    "url=\"http://20.79.233.37:5015/api/ais/ImageClassification/VulnerabiltyReport\"\n",
    "headers={'Cache-Control': 'no-cache',\n",
    "'Ocp-Apim-Subscription-Key': \"646b6b8560a042c49d3af308738d6f9c\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Files path\n",
    "\"\"\"\n",
    "data_path=os.path.join(zip_path,'data.zip') #full path of data zip\n",
    "label_path=os.path.join(zip_path,'label.zip') #full path of label zip\n",
    "model_path=os.path.join(zip_path,'model.zip') #full path of model zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Payload for AIShield VulnerabilityReport api call\n",
    "\"\"\"\n",
    "payload={}\n",
    "payload['input_dimensions']=str(input_shape)\n",
    "payload['number_of_classes']=str(num_classes)\n",
    "payload['attack_type']=\"blackbox\"\n",
    "payload['number_of_attack_queries']=50000\n",
    "payload['model_framework']='tensorflow'\n",
    "payload['vulnerability_threshold']=\"0\"\n",
    "payload['normalize_data']=\"yes\"\n",
    "payload['defense_bestonly']=\"no\"\n",
    "payload['encryption_strategy']= model_encryption\n",
    "payload['model_api_details']=\"no\"\n",
    "payload['use_model_api'] = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Files for AIShield VulnerabilityReport api call\n",
    "\"\"\"\n",
    "model_name=os.path.split(model_path)[1] #model file name\n",
    "data_name=os.path.split(data_path)[1] #data file name\n",
    "label_name=os.path.split(label_path)[1] #label file name\n",
    "\n",
    "files=[\n",
    "  ('data_zip',(data_name,open(data_path,'rb'),'application/x-zip-compressed')),\n",
    "  ('model_zip',(model_name,open(model_path,'rb'),'application/x-zip-compressed')),\n",
    "  ('label_zip',(label_name,open(label_path,'rb'),'application/x-zip-compressed'))\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Hit AIShield VulnerabilityReport api\n",
    "\"\"\"\n",
    "new_request = requests.request(method=\"POST\", url=url, params=payload, files=files,headers=headers)\n",
    "new_request=json.loads(new_request.text)\n",
    "for k, v in new_request.items():\n",
    "    print(\"* {} : {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Get job id from api response\n",
    "\"\"\"\n",
    "job_id=new_request['job_id']\n",
    "print(f\"Job id : {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16203113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Query GetProcessDetail api and Monitor progress for job id\n",
    "\"\"\"\n",
    "job_status_url=url[:-18]+\"JobStatusDetailed?JobID=\" +job_id\n",
    "\n",
    "#status dictionary\n",
    "status_dictionary={\n",
    " 'ModelExploration_Status': 'na',\n",
    " 'SanityCheck_Status': 'na',\n",
    " 'QueryGenerator_Status': 'na',\n",
    " 'VunerabilityEngine_Status': 'na', \n",
    " 'DefenseReport_Status': 'na',\n",
    " 'IntegratedModelGenerator_Status':'na',\n",
    "\n",
    "}\n",
    "while(True):\n",
    "    time.sleep(5) \n",
    "    job_status_response = requests.request(\"GET\", job_status_url, params={},headers=headers)\n",
    "\n",
    "    job_status_payload=json.loads(job_status_response.text)\n",
    "    failing_key='ModelExploration_Status'\n",
    "    for key in status_dictionary.keys():\n",
    "        if status_dictionary[key]=='na':\n",
    "            if job_status_payload[key]=='completed' or job_status_payload[key]=='passed':\n",
    "                status_dictionary[key]=job_status_payload[key]\n",
    "                print(str(key), \":\",status_dictionary[key])\n",
    "            elif job_status_payload[key]=='failed':\n",
    "                failing_key=key\n",
    "                status_dictionary[key]=job_status_payload[key]\n",
    "                print(str(key), \":\",status_dictionary[key])\n",
    "\n",
    "    if status_dictionary[failing_key]=='failed':\n",
    "        break\n",
    "\n",
    "    if status_dictionary['VunerabilityEngine_Status']=='passed' or status_dictionary['VunerabilityEngine_Status']=='completed' and job_status_payload['CurrentStatus']==\"Defense generation is not triggered\" :\n",
    "        print(\"\\n Vulnerability score {} failed to cross vulnerability threshoold of {}\".format(job_status_payload['VulnerabiltyScore'],job_meta_data['vulnerability_threshold']))\n",
    "        break\n",
    "    if job_status_payload['IntegratedModelGenerator_Status']=='completed':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ca13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Description: Response from GetProcessDetail api\n",
    "\"\"\"\n",
    "for k, v in job_status_payload.items():\n",
    "    print(\"* {} : {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Description: API to Get Job Details\n",
    "\"\"\"\n",
    "url_meta = url[:-18] + \"/JobDetail?JobID=\" + job_id\n",
    "\n",
    "response = requests.request(\"GET\", url_meta, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Description: API to Get Report for given Job Details\n",
    "\"\"\"\n",
    "url_report = url[:-18]+\"/GetReport?JobID=\" + job_id + \"&ReportType=defense&FileFormat=1\"\n",
    "\n",
    "response = requests.request(\"GET\", url_report, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d434b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bcae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
